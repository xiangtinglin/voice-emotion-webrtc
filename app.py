import streamlit as st
from streamlit_audio_recorder import audio_recorder
import tempfile
import torchaudio
from transformers import pipeline
from gtts import gTTS
import os

st.set_page_config(page_title="ğŸ¤ Voice Emotion Chatbot", layout="centered")
st.title("ğŸ™ï¸ Voice Emotion Chatbot Demo")

# éŒ„éŸ³å€å¡Š
st.markdown("## Step 1: è«‹éŒ„éŸ³")
audio_bytes = audio_recorder(pause_threshold=3.0)

# å¦‚æœæœ‰éŒ„åˆ°è²éŸ³
if audio_bytes:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(audio_bytes)
        audio_path = tmp.name

    st.success("âœ… éŒ„éŸ³å®Œæˆï¼Œé–‹å§‹è¾¨è­˜ä¸­...")

    # Whisper èªéŸ³è¾¨è­˜
    pipe = pipeline("automatic-speech-recognition", model="openai/whisper-small")
    result = pipe(audio_path)
    text = result["text"]
    st.markdown(f"**ä½ èªªçš„æ˜¯ï¼š** {text}")

    # æƒ…ç·’åˆ†æ
    classifier = pipeline("sentiment-analysis")
    sentiment = classifier(text)[0]
    st.markdown(f"**åµæ¸¬åˆ°çš„æƒ…ç·’ï¼š** `{sentiment['label']}`ï¼ˆä¿¡å¿ƒå€¼ï¼š{sentiment['score']:.2f}ï¼‰")

    # å›æ‡‰èªéŸ³
    response = f"ä½ è½èµ·ä¾†æœ‰é» {sentiment['label']}ï¼Œä¸€åˆ‡é‚„å¥½å—ï¼Ÿ"
    tts = gTTS(response, lang="zh")
    tts_path = os.path.join(tempfile.gettempdir(), "response.mp3")
    tts.save(tts_path)

    st.markdown("### èªéŸ³å›æ‡‰ï¼š")
    st.audio(tts_path)
