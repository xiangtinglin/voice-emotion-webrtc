import streamlit as st
import streamlit.components.v1 as components
import tempfile
import base64
import os
from transformers import pipeline
from gtts import gTTS

st.set_page_config(page_title="ğŸ¤ Whisper Voice Chat", layout="centered")
st.title("ğŸ—£ï¸ Whisper èªéŸ³æƒ…ç·’èŠå¤©æ©Ÿå™¨äºº")

# éŒ„éŸ³ HTML å…ƒä»¶
st.markdown("### Step 1: éŒ„éŸ³")
components.html("""
    <script>
    let mediaRecorder;
    let audioChunks = [];
    let startBtn, stopBtn;

    function createRecorderUI() {
        const container = document.createElement("div");

        startBtn = document.createElement("button");
        startBtn.innerText = "ğŸ™ï¸ é–‹å§‹éŒ„éŸ³";
        startBtn.onclick = startRecording;

        stopBtn = document.createElement("button");
        stopBtn.innerText = "ğŸ›‘ åœæ­¢éŒ„éŸ³";
        stopBtn.onclick = stopRecording;
        stopBtn.disabled = true;

        container.appendChild(startBtn);
        container.appendChild(stopBtn);
        document.body.appendChild(container);
    }

    async function startRecording() {
        audioChunks = [];
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.start();
        mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
        mediaRecorder.onstop = () => {
            const blob = new Blob(audioChunks, { type: 'audio/wav' });
            const reader = new FileReader();
            reader.onloadend = () => {
                const base64Audio = reader.result.split(',')[1];
                const form = document.createElement('form');
                form.method = 'POST';
                form.action = '/';
                const input = document.createElement('input');
                input.type = 'hidden';
                input.name = 'audio';
                input.value = base64Audio;
                form.appendChild(input);
                document.body.appendChild(form);
                form.submit();
            };
            reader.readAsDataURL(blob);
        };
        startBtn.disabled = true;
        stopBtn.disabled = false;
    }

    function stopRecording() {
        mediaRecorder.stop();
        startBtn.disabled = false;
        stopBtn.disabled = true;
    }

    createRecorderUI();
    </script>
""", height=150)

# æ¥æ”¶éŸ³è¨Š
if "audio" in st.experimental_get_query_params():
    audio_b64 = st.experimental_get_query_params()["audio"][0]
    audio_bytes = base64.b64decode(audio_b64)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
        f.write(audio_bytes)
        audio_path = f.name

    st.success("âœ… éŒ„éŸ³æ¥æ”¶æˆåŠŸï¼Œé–‹å§‹èªéŸ³è¾¨è­˜ä¸­...")

    # Whisper æ¨¡å‹
    asr = pipeline("automatic-speech-recognition", model="openai/whisper-small")
    result = asr(audio_path)
    text = result["text"]

    st.markdown(f"**ä½ èªªçš„æ˜¯ï¼š** {text}")

    # æƒ…ç·’åˆ†æ
    clf = pipeline("sentiment-analysis")
    sentiment = clf(text)[0]
    st.markdown(f"**åµæ¸¬åˆ°æƒ…ç·’ï¼š** `{sentiment['label']}`ï¼ˆä¿¡å¿ƒå€¼ï¼š{sentiment['score']:.2f}ï¼‰")

    # å›æ‡‰èªéŸ³
    response = f"ä½ è½èµ·ä¾†æœ‰äº› {sentiment['label']}ï¼Œè¦ä¸è¦èŠèŠå‘¢ï¼Ÿ"
    tts = gTTS(response, lang='zh')
    tts_path = os.path.join(tempfile.gettempdir(), "response.mp3")
    tts.save(tts_path)

    st.audio(tts_path)
